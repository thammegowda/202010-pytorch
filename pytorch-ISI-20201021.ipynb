{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyTorch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch torchtext "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep a tab open for pytorch docs : https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:device=cuda\n"
     ]
    }
   ],
   "source": [
    "import logging as log\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "log.basicConfig(level=log.INFO)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f'device={device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtext\n",
    "torchtext docs refer to https://pytorch.org/text/experimental_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Downloading from Google Drive; may take a few minutes\n",
      "INFO:root:File cache-torchtext/ag_news_csv.tar.gz already exists.\n",
      "INFO:root:Opening tar file cache-torchtext/ag_news_csv.tar.gz.\n",
      "INFO:root:cache-torchtext/ag_news_csv/train.csv already extracted.\n",
      "INFO:root:cache-torchtext/ag_news_csv/test.csv already extracted.\n",
      "INFO:root:cache-torchtext/ag_news_csv/classes.txt already extracted.\n",
      "INFO:root:cache-torchtext/ag_news_csv/readme.txt already extracted.\n",
      "120000lines [00:01, 111867.30lines/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torchtext.experimental.datasets.text_classification.TextClassificationDataset,\n",
       " torchtext.experimental.datasets.text_classification.TextClassificationDataset)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.experimental.datasets import YahooAnswers, DBpedia, AG_NEWS\n",
    "\n",
    "root = 'cache-torchtext'\n",
    "#train, test = YahooAnswers(root=root)\n",
    "#train, valid = DBpedia(root=root)\n",
    "train, valid = AG_NEWS(root=root)   # using this because it is small\n",
    "type(train), type(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader with Mini Batches\n",
    "\n",
    "- Torch's [datatypes](https://pytorch.org/docs/stable/tensors.html#torch-tensor)\n",
    "- [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor(3), tensor(3), tensor(4)]\n",
      "[tensor([ 2695,   370, 10586,  1095,    26,    13,    10,     6,  5300, 13907,\n",
      "           13,    10,  9045,  9358,  4345,   378,   370,    60,     7,     6,\n",
      "        14337,  5258,     9,  2385,   253,     3,  5874,     2,   903,   234,\n",
      "           31,  6861,     4, 22318,  2347,    18,    94,   970]), tensor([  307,   976,  1150,   233,     5,  2834,   113,   226,    50,  1150,\n",
      "           13,   279,    21,   976,   324,  1062,    27,    70,   116,    18,\n",
      "           31,    45, 10844,   238,    45,   438,     7,  1150,   974,     5,\n",
      "         2834,   259,     8,   451,   303,     7,   761,    69,     3,   788,\n",
      "           13,    10, 10500,   414,  2365,     2]), tensor([49562,   617,     5,   355,  5697,     4,   163,     8, 10297,   755,\n",
      "            6,   734,     7, 49562,  2007,    99,    65,     2,   299,    58,\n",
      "            5,   355,  5697,     4,   163,     5,     3,   195,   101,     5,\n",
      "         1105,  1872,    18,     3, 13627,   286,  1374,   213, 13008,   673,\n",
      "            8,  6770,     7,     3,   185, 10297,  8670,     2,    42,  1466,\n",
      "         1543,     4,  1178,  1430,  1548,     4,  1540,     4,  1544,  2716,\n",
      "         1541,    42,   165,  2799,   145,  2752,   164,    42,  1533])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = [], []\n",
    "    for label, txt in batch:\n",
    "        texts.append(txt)\n",
    "        labels.append(label)        \n",
    "    return texts, labels\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=3, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "for idx, (texts, labels) in enumerate(train_loader):\n",
    "    print(idx, labels)\n",
    "    print(texts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have unequal length sequences:\n",
    "- use padding\n",
    "- keep track of lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:vocabulary size= 95,812\n",
      "INFO:root:<unk>=0\n",
      "INFO:root:<pad>=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 30000, 4: 30000, 2: 30000, 1: 30000})\n"
     ]
    }
   ],
   "source": [
    "# vocabul\n",
    "train.vocab.itos[:10]\n",
    "PAD_IDX = train.vocab.stoi['<pad>']\n",
    "UNK_IDX = train.vocab.stoi['<unk>']\n",
    "log.info(f'vocabulary size= {len(train.vocab):,}')\n",
    "log.info(f'<unk>={UNK_IDX}')\n",
    "log.info(f'<pad>={PAD_IDX}')\n",
    "\n",
    "from collections import Counter\n",
    "all_labels = Counter(label for label, txt in train.data)\n",
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([2, 1, 3], dtype=torch.uint8) tensor([27, 35, 35], dtype=torch.int16)\n",
      "tensor([[    6,    17,    10,   916,  9506,  9426,  3603,  5293,  1720,  9506,\n",
      "            29, 13413,     6, 29183,    18,    94,  1319,   284,    30,     3,\n",
      "          1601,  2817,     5,     3,  1387,  2382,     2,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1],\n",
      "        [ 1634,   673,    12,    24,  2039,     5,  1218,  1220,     3,  1558,\n",
      "           214,     4,   334,  2606,  1634,     4,    29,   441,    12,     6,\n",
      "            24,  2039,     5,  5633,     3, 21941,  1220,    19,   197,    39,\n",
      "             3,  2410,     7,  1218,     2],\n",
      "        [   37,   254,  1800,    67,  6355, 20892,     6,    37,   930,  2979,\n",
      "            99,  1376,    27,    26,    87,  2074,     6,  2051,   746,     8,\n",
      "          3561,  8028,  5720,  6355, 20892,     8,     6,   122, 14697,     3,\n",
      "            55,    21,  9588,    83,     2]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts, labels = [], []\n",
    "    for label, txt in batch:\n",
    "        texts.append(txt)\n",
    "        labels.append(label)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.uint8)    \n",
    "    lengths = [len(txt) for txt in texts]\n",
    "    lengths = torch.tensor(lengths, dtype=torch.short)\n",
    "\n",
    "    seqs = torch.full(size=(len(texts), lengths.max()),\n",
    "                      fill_value=PAD_IDX, dtype=torch.long)\n",
    "    for idx, txt in enumerate(texts):\n",
    "        seqs[idx, :len(txt)] = txt\n",
    "    \n",
    "    return seqs, labels, lengths\n",
    "train_loader = DataLoader(train, batch_size=3, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "for idx, (texts, labels, lengths) in enumerate(train_loader):\n",
    "    print(idx, labels, lengths)\n",
    "    print(texts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([2, 3, 1], dtype=torch.uint8) tensor([38, 48, 19], dtype=torch.int16)\n",
      "tensor([[77866,   159,   243,   621,    44,  1376,     8,     3,    90, 17327,\n",
      "            20,     6,   446,   111,   231,  4091,  1882,  8017,  5643,  3778,\n",
      "             9,  4029,  4592,     4,     3,   434,  4080,     3,  1501,    41,\n",
      "           250,  1376,    40,     6,  1982,     8,  1933,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [  341,   109,  2707,   338,  1380,  1043,  1531,   873,  1364,  2707,\n",
      "            27,    26,   603, 68465,    83,    14, 93379,    83,    15,     8,\n",
      "             3,    48,   512,     7,     3,    72,     4,   685,  1113,     3,\n",
      "           788,    13,    10, 68451,    83,    14, 93339,    83,    15,  1043,\n",
      "            12,     3,   730,  1740,     3,  1602,    72,     2],\n",
      "        [ 1029,  4734, 11178,   146,   197,    17,    10,  5777,  4734,  1507,\n",
      "             3,   146,    12,   290,  7693,     4,    64,  1945,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "\n",
    "    texts: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    lengths: torch.Tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(seqs) \n",
    "\n",
    "    def tok_count(self):\n",
    "        return self.lengths.sum()\n",
    "\n",
    "    def to(self, device):\n",
    "        # there is an option called pin_memory for advanced users\n",
    "        self.texts = self.texts.to(device)\n",
    "        self.labels = self.labels.to(device)\n",
    "        self.lengths = self.lengths.to(device)        \n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def collate_fn(cls, batch, max_len=80) -> 'Batch':\n",
    "        # handle edge cases, define max_len \n",
    "        texts, labels = [], []\n",
    "        for label, txt in batch:\n",
    "            texts.append(txt[:max_len])\n",
    "            labels.append(label)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.uint8)    \n",
    "        lengths = [len(txt) for txt in texts]\n",
    "        lengths = torch.tensor(lengths, dtype=torch.short)\n",
    "\n",
    "        seqs = torch.full(size=(len(texts), lengths.max()),\n",
    "                          fill_value=PAD_IDX, dtype=torch.long)\n",
    "        for idx, txt in enumerate(texts):\n",
    "            seqs[idx, :len(txt)] = txt\n",
    "    \n",
    "        return cls(texts=seqs, labels=labels, lengths=lengths)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=3, collate_fn=Batch.collate_fn, shuffle=True)\n",
    "\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(idx, batch.labels, batch.lengths)\n",
    "    print(batch.texts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37msaga22                \u001b[m  Tue Oct 20 17:16:07 2020  \u001b[1m\u001b[30m440.44\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mGeForce GTX 1080 Ti\u001b[m |\u001b[31m 34'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   10\u001b[m / \u001b[33m11178\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tqdm`\n",
    "\n",
    "tqdm provides progress bar https://github.com/tqdm/tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 99/40000 [00:00<01:14, 538.09it/s, updates=100, epoch=0, toks_speed=69,922]\n",
      "INFO:root:Epoch 0 completed\n",
      "  0%|          | 99/40000 [00:00<00:58, 677.43it/s, updates=200, epoch=1, toks_speed=77,262]\n",
      "INFO:root:Epoch 1 completed\n",
      "  0%|          | 99/40000 [00:00<00:46, 856.26it/s, updates=300, epoch=2, toks_speed=85,590]\n",
      "INFO:root:Epoch 2 completed\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time as time_now\n",
    "\n",
    "steps = 100000\n",
    "\n",
    "count = 0\n",
    "epoch = 0\n",
    "total_toks = 0\n",
    "start_time = time_now()\n",
    "while count < steps:\n",
    "    # https://github.com/tqdm/tqdm#documentation \n",
    "    with tqdm(train_loader) as pbar:\n",
    "        for batch in pbar:\n",
    "            count += 1\n",
    "            total_toks += batch.tok_count()\n",
    "            toks_speed = int(total_toks / (time_now() - start_time))\n",
    "            pbar.set_postfix(dict(updates=count, epoch=epoch, toks_speed=f'{toks_speed:,}'))\n",
    "            if count % 100 == 0:  # testing \n",
    "                break\n",
    "    log.info(f'Epoch {epoch} completed')\n",
    "    epoch += 1\n",
    "    if epoch > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 999/100000 [00:01<02:03, 802.92it/s, speed=103,930toks/sec]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time as time_now\n",
    "\n",
    "def looping_iter(data, total):\n",
    "    \"\"\"Flattens multiple epoch loops into one \"\"\"\n",
    "    count = 0\n",
    "    epoch = 0\n",
    "    while count < total:\n",
    "        for item in data:\n",
    "            yield item\n",
    "            count += 1\n",
    "            if count >= total:\n",
    "                break\n",
    "        if count < total:\n",
    "            log.info(f\"End of epoch {epoch}\")\n",
    "        epoch += 1\n",
    "\n",
    "steps = 100_000\n",
    "total_toks = 0\n",
    "start_time = time_now()\n",
    "with tqdm(looping_iter(train_loader, total=steps), total=steps) as pbar:\n",
    "    for count, batch in enumerate(pbar):\n",
    "        total_toks += batch.tok_count()\n",
    "        toks_speed = int(total_toks / (time_now() - start_time))\n",
    "        pbar.set_postfix_str(f'speed={toks_speed:,}toks/sec')\n",
    "        if (count + 1) % 1000 == 0:  # testing \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim\n",
    "\n",
    "@dataclass\n",
    "class Trainer:\n",
    "    model: nn.Module\n",
    "    opt: optim.Optimizer\n",
    "    #loss_func = nn.CrossEntropyLoss()      # object oriented API\n",
    "    loss_func = F.cross_entropy             # functional API \n",
    "\n",
    "# I use object-oriented API for components with states\n",
    "#    and functional API for stateless components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 187/1000 [00:00<00:03, 219.74it/s, 351,262toks/sec]INFO:root:TODO: checkpoint at step: 200\n",
      " 39%|███▉      | 394/1000 [00:01<00:02, 233.77it/s, 382,230toks/sec]INFO:root:TODO: checkpoint at step: 400\n",
      " 60%|█████▉    | 599/1000 [00:02<00:01, 215.95it/s, 377,730toks/sec]INFO:root:TODO: checkpoint at step: 600\n",
      " 79%|███████▊  | 787/1000 [00:03<00:00, 239.25it/s, 381,976toks/sec]INFO:root:TODO: checkpoint at step: 800\n",
      "100%|█████████▉| 995/1000 [00:04<00:00, 211.97it/s, 378,886toks/sec]INFO:root:TODO: checkpoint at step: 1,000\n",
      "100%|█████████▉| 999/1000 [00:04<00:00, 220.32it/s, 378,886toks/sec]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time as time_now\n",
    "\n",
    "  \n",
    "@dataclass\n",
    "class Trainer:\n",
    "    model: nn.Module\n",
    "    opt: optim.Optimizer = None\n",
    "    loss_func = F.cross_entropy\n",
    "    device = device\n",
    "\n",
    "    def train(self, train_loader: DataLoader, valid_loader: DataLoader, steps: int, checkpoint: int):\n",
    "        total_toks = 0\n",
    "        start_time = time_now()\n",
    "        train_data = looping_iter(train_loader, total=steps)\n",
    "        with tqdm(train_data, total=steps) as databar:\n",
    "            for step, batch in enumerate(databar, start=1):\n",
    "                total_toks += batch.tok_count()\n",
    "                toks_speed = int(total_toks / (time_now() - start_time))\n",
    "\n",
    "                databar.set_postfix_str(f'{toks_speed:,}toks/sec')\n",
    "                # TODO: train \n",
    "\n",
    "                if step % checkpoint == 0:\n",
    "                    log.info(f\"TODO: checkpoint at step: {step:,}\")\n",
    "\n",
    "                if step >= steps:\n",
    "                    break\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=Batch.collate_fn)\n",
    "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Batch.collate_fn)\n",
    "\n",
    "trainer = Trainer(model=None, opt=None)\n",
    "trainer.train(train_loader, valid_loader, steps=1000, checkpoint=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TextClassifier\n",
    "\n",
    "vocab = train.vocab\n",
    "n_classes = max(all_labels) + 1\n",
    "model_args = dict(vocab_size=len(vocab), n_classes=n_classes,\n",
    "                  model_dim=128, n_heads=2, n_layers=2, ff_dim=256,\n",
    "                  padding_idx=PAD_IDX)\n",
    "# Note: save model_args somewhere\n",
    "model = TextClassifier(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time as time_now\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, lr=5e-4, device=device,\n",
    "                 opt=None, loss_func=None, lr_scheduler=None):\n",
    "        self.device = device        \n",
    "        self.model = model.to(self.device)\n",
    "        self.opt = opt or optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_func = loss_func or F.cross_entropy\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        \n",
    "    def validate(self, valid_loader) -> float:\n",
    "        total = 0.\n",
    "        count = 0\n",
    "        for batch in valid_loader:\n",
    "            batch.to(device)\n",
    "            scores = self.model(texts=batch.texts, lengths=batch.lengths, out='raw')\n",
    "            loss = self.loss_func(input=scores, target=batch.labels.long(), reduction='mean')\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "        return total / count\n",
    "\n",
    "    def checkpoint(self):\n",
    "        pass\n",
    "\n",
    "    def train(self, train_loader: DataLoader, valid_loader: DataLoader, steps: int, checkpoint: int):\n",
    "        train_loss = 0.\n",
    "        total_toks = 0\n",
    "        start_time = time_now()\n",
    "        self.model.train(True) #Training mode\n",
    "        train_data = looping_iter(train_loader, total=steps)\n",
    "        with tqdm(train_data, total=steps) as databar:\n",
    "            for step, batch in enumerate(databar, start=1):\n",
    "                batch.to(self.device)\n",
    "\n",
    "                scores = self.model(texts=batch.texts, lengths=batch.lengths, out='raw')\n",
    "                # NOTE: loss_func accepts long values for target \n",
    "                loss = self.loss_func(input=scores, target=batch.labels.long(), reduction='mean')\n",
    "\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                loss_val = loss.detach().item()\n",
    "                train_loss += loss_val\n",
    "                total_toks += batch.tok_count()\n",
    "                toks_speed = int(total_toks / (time_now() - start_time))\n",
    "\n",
    "                databar.set_postfix_str(f'loss:{loss_val:.4f} speed:{toks_speed:,}toks/sec', refresh=False)\n",
    "\n",
    "                if step % checkpoint == 0:\n",
    "                    with torch.no_grad():\n",
    "                        self.model.train(False)\n",
    "                        val_loss = self.validate(valid_loader)\n",
    "                        train_loss /= checkpoint\n",
    "                        log.info(f'\\nCheckpoint at {step}; train_loss={train_loss:.4f} valid_loss={val_loss:.4f}')\n",
    "                        # TODO: checkpoint\n",
    "                        self.model.train(True)\n",
    "                        train_loss = 0.\n",
    "                        start_time = time_now()\n",
    "                        total_toks = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 600\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=Batch.collate_fn)\n",
    "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Batch.collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIP: Check if the model can overfit to smaller dataset\n",
    "\n",
    " _smaller dataset_ => 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model)\n",
    "batch1 = next(iter(train_loader))\n",
    "trainer.train(train_loader=[batch1], valid_loader=valid_loader, steps=10000, checkpoint=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed on CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check speed on CPU \n",
    "trainer2 = Trainer(model=model, device='cpu')\n",
    "trainer2.train(train_loader=train_loader, valid_loader=valid_loader, steps=10000, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, device='cuda:0')\n",
    "trainer.train(train_loader=train_loader, valid_loader=valid_loader, steps=10000, checkpoint=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 600\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=Batch.collate_fn)\n",
    "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Batch.collate_fn)\n",
    "trainer = Trainer(model=model, device='cuda:0')\n",
    "trainer.train(train_loader=train_loader, valid_loader=valid_loader, steps=10000, checkpoint=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Tensorboard\n",
    "\n",
    "part of `torch.utils` package\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "```\n",
    "The class takes these args\n",
    "```python\n",
    "SummaryWriter(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')\n",
    "```\n",
    "\n",
    "and offers these functions:\n",
    "\n",
    "```python\n",
    "add_scalar(tag, scalar_value, global_step=None, walltime=None)\n",
    "add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)\n",
    "add_graph(model, input_to_model=None, verbose=False)\n",
    "add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)\n",
    "\n",
    "add_text(tag, text_string, global_step=None, walltime=None)\n",
    "add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)\n",
    "add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')\n",
    "add_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW')\n",
    "add_figure(tag, figure, global_step=None, close=True, walltime=None)\n",
    "add_video(tag, vid_tensor, global_step=None, fps=4, walltime=None)\n",
    "add_audio(tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None)\n",
    "add_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None)\n",
    "add_hparams(hparam_dict=None, metric_dict=None)\n",
    "\n",
    "```\n",
    "\n",
    "Install tensorboard: `pip install tensorboard`\n",
    "\n",
    "To launch tensorboard \n",
    "```bash\n",
    "tensorboard --logdir <dirname> # --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, lr=5e-4, device=device,\n",
    "                 opt=None, loss_func=None, lr_scheduler=None):\n",
    "        self.device = device        \n",
    "        self.model = model.to(self.device)\n",
    "        self.opt = opt or optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_func = loss_func or F.cross_entropy\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.tbd = SummaryWriter(log_dir='tensorboard-logs')    # 👈 \n",
    "        \n",
    "    def validate(self, valid_loader) -> float:\n",
    "        total = 0.\n",
    "        count = 0\n",
    "        for batch in valid_loader:\n",
    "            batch.to(device)\n",
    "            scores = self.model(texts=batch.texts, lengths=batch.lengths, out='raw')\n",
    "            loss = self.loss_func(input=scores, target=batch.labels.long(), reduction='mean')\n",
    "            total += loss.item()\n",
    "            count += 1\n",
    "        return total / count\n",
    "    \n",
    "    def log_model_graph(self, batch):\n",
    "        # input is needed to extract the dynamic graph \n",
    "        self.tbd.add_graph(self.model, (batch.texts, batch.lengths))  #👈\n",
    "\n",
    "\n",
    "    def train(self, train_loader: DataLoader, valid_loader: DataLoader, steps: int, checkpoint: int):        \n",
    "        inp_emb_names = train_loader.dataset.vocab.itos\n",
    "        train_loss = 0.\n",
    "        total_toks = 0\n",
    "        start_time = time_now()\n",
    "        self.model.train(True) #Training mode\n",
    "        train_data = looping_iter(train_loader, total=steps)\n",
    "        with tqdm(train_data, total=steps) as databar:\n",
    "            for step, batch in enumerate(databar, start=1):\n",
    "                batch.to(self.device)\n",
    "                if step == 1:                            # 👈\n",
    "                    self.log_model_graph(batch) \n",
    "\n",
    "                scores = self.model(texts=batch.texts, lengths=batch.lengths, out='raw')\n",
    "                # NOTE: loss_func accepts long values for target \n",
    "                loss = self.loss_func(input=scores, target=batch.labels.long(), reduction='mean')\n",
    "\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                loss_val = loss.detach().item()\n",
    "                train_loss += loss_val \n",
    "                self.tbd.add_scalar('train_loss', loss_val, step)    # 👈\n",
    "\n",
    "                step += 1\n",
    "                databar.set_postfix(dict(updates=count, loss=loss_val), refresh=False)\n",
    "                if step % checkpoint == 0:\n",
    "                    with torch.no_grad():\n",
    "                        self.model.train(False)\n",
    "                        val_loss = self.validate(valid_loader)\n",
    "                        train_loss /= checkpoint\n",
    "                        #👇\n",
    "                        self.tbd.add_scalars('chkpt_losses', {'train': train_loss, 'valid': val_loss}, step)  \n",
    "                        #self.tbd.add_embedding(mat=self.model.embeddings[0].weight, global_step=step, metadata=inp_emb_names,\n",
    "                        #                       tag='input_embedding')\n",
    "                        self.tbd.add_embedding(mat=self.model.cls_proj.weight, global_step=step, tag='output_embedding')\n",
    "\n",
    "                        log.info(f'\\nCheckpoint at {step}; train_loss={train_loss:.4f} valid_loss={val_loss:.4f}')\n",
    "                        # TODO: checkpoint\n",
    "                        self.model.train(True)\n",
    "                        train_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 92/10000 [00:01<03:30, 47.17it/s, updates=999, loss=1.12] INFO:root:\n",
      "Checkpoint at 100; train_loss=1.1610 valid_loss=1.0044\n",
      "  2%|▏         | 195/10000 [00:04<02:13, 73.54it/s, updates=999, loss=0.724]INFO:root:\n",
      "Checkpoint at 200; train_loss=0.9588 valid_loss=0.8613\n",
      "  3%|▎         | 295/10000 [00:07<02:11, 74.04it/s, updates=999, loss=0.67] INFO:root:\n",
      "Checkpoint at 300; train_loss=0.8816 valid_loss=0.7810\n",
      "  4%|▍         | 391/10000 [00:10<02:18, 69.59it/s, updates=999, loss=0.746]INFO:root:\n",
      "Checkpoint at 400; train_loss=0.8305 valid_loss=0.7270\n",
      "  5%|▍         | 496/10000 [00:13<02:01, 77.96it/s, updates=999, loss=0.727]INFO:root:\n",
      "Checkpoint at 500; train_loss=0.7148 valid_loss=0.6803\n",
      "  6%|▌         | 595/10000 [00:16<02:00, 77.88it/s, updates=999, loss=0.551]INFO:root:\n",
      "Checkpoint at 600; train_loss=0.6872 valid_loss=0.6246\n",
      "  7%|▋         | 692/10000 [00:18<02:06, 73.44it/s, updates=999, loss=0.634]INFO:root:\n",
      "Checkpoint at 700; train_loss=0.6682 valid_loss=0.6210\n",
      "  8%|▊         | 794/10000 [00:21<02:04, 73.77it/s, updates=999, loss=0.331]INFO:root:\n",
      "Checkpoint at 800; train_loss=0.6374 valid_loss=0.5909\n",
      "  9%|▉         | 893/10000 [00:24<02:08, 71.06it/s, updates=999, loss=0.542]INFO:root:\n",
      "Checkpoint at 900; train_loss=0.6189 valid_loss=0.5317\n",
      " 10%|▉         | 994/10000 [00:27<02:01, 74.03it/s, updates=999, loss=0.42] INFO:root:\n",
      "Checkpoint at 1000; train_loss=0.5623 valid_loss=0.5503\n",
      " 11%|█         | 1090/10000 [00:30<02:02, 73.02it/s, updates=999, loss=0.853]INFO:root:\n",
      "Checkpoint at 1100; train_loss=0.5647 valid_loss=0.5186\n",
      " 12%|█▏        | 1194/10000 [00:33<01:59, 73.98it/s, updates=999, loss=0.596]INFO:root:\n",
      "Checkpoint at 1200; train_loss=0.5729 valid_loss=0.4807\n",
      " 13%|█▎        | 1291/10000 [00:35<01:59, 73.18it/s, updates=999, loss=0.75] INFO:root:\n",
      "Checkpoint at 1300; train_loss=0.5356 valid_loss=0.4814\n",
      " 14%|█▍        | 1393/10000 [00:38<02:00, 71.22it/s, updates=999, loss=0.413]INFO:root:\n",
      "Checkpoint at 1400; train_loss=0.5191 valid_loss=0.4776\n",
      " 15%|█▍        | 1498/10000 [00:41<01:54, 74.04it/s, updates=999, loss=0.299]INFO:root:\n",
      "Checkpoint at 1500; train_loss=0.5137 valid_loss=0.5092\n",
      " 16%|█▌        | 1594/10000 [00:44<01:58, 70.95it/s, updates=999, loss=0.822]INFO:root:\n",
      "Checkpoint at 1600; train_loss=0.5159 valid_loss=0.4674\n",
      " 17%|█▋        | 1696/10000 [00:46<01:53, 72.85it/s, updates=999, loss=0.321]INFO:root:\n",
      "Checkpoint at 1700; train_loss=0.4830 valid_loss=0.4629\n",
      " 18%|█▊        | 1796/10000 [00:49<01:48, 75.52it/s, updates=999, loss=0.927]INFO:root:\n",
      "Checkpoint at 1800; train_loss=0.4789 valid_loss=0.4515\n",
      " 19%|█▉        | 1894/10000 [00:52<01:47, 75.42it/s, updates=999, loss=0.501]INFO:root:\n",
      "Checkpoint at 1900; train_loss=0.5085 valid_loss=0.4623\n",
      " 20%|█▉        | 1992/10000 [00:55<01:51, 71.63it/s, updates=999, loss=0.44] INFO:root:\n",
      "Checkpoint at 2000; train_loss=0.4779 valid_loss=0.4629\n",
      " 21%|██        | 2098/10000 [00:58<01:43, 76.15it/s, updates=999, loss=0.319]INFO:root:\n",
      "Checkpoint at 2100; train_loss=0.5216 valid_loss=0.4479\n",
      " 22%|██▏       | 2192/10000 [01:00<01:51, 69.89it/s, updates=999, loss=0.362]INFO:root:\n",
      "Checkpoint at 2200; train_loss=0.4472 valid_loss=0.4275\n",
      " 23%|██▎       | 2292/10000 [01:03<01:42, 74.88it/s, updates=999, loss=0.771]INFO:root:\n",
      "Checkpoint at 2300; train_loss=0.4775 valid_loss=0.4237\n",
      " 24%|██▍       | 2390/10000 [01:06<01:41, 74.98it/s, updates=999, loss=0.514]INFO:root:\n",
      "Checkpoint at 2400; train_loss=0.4797 valid_loss=0.4194\n",
      " 25%|██▍       | 2496/10000 [01:09<01:37, 76.64it/s, updates=999, loss=0.465] INFO:root:\n",
      "Checkpoint at 2500; train_loss=0.4581 valid_loss=0.4081\n",
      " 26%|██▌       | 2592/10000 [01:11<01:38, 75.43it/s, updates=999, loss=0.332]INFO:root:\n",
      "Checkpoint at 2600; train_loss=0.4396 valid_loss=0.4042\n",
      " 27%|██▋       | 2698/10000 [01:14<01:34, 77.15it/s, updates=999, loss=0.366]INFO:root:\n",
      "Checkpoint at 2700; train_loss=0.4305 valid_loss=0.4206\n",
      " 28%|██▊       | 2792/10000 [01:17<01:38, 73.05it/s, updates=999, loss=0.303]INFO:root:\n",
      "Checkpoint at 2800; train_loss=0.4144 valid_loss=0.4110\n",
      " 29%|██▉       | 2890/10000 [01:20<01:33, 76.17it/s, updates=999, loss=0.322]INFO:root:\n",
      "Checkpoint at 2900; train_loss=0.3976 valid_loss=0.4190\n",
      " 30%|██▉       | 2995/10000 [01:23<01:30, 77.13it/s, updates=999, loss=0.326]INFO:root:\n",
      "Checkpoint at 3000; train_loss=0.4181 valid_loss=0.3847\n",
      " 31%|███       | 3092/10000 [01:25<01:35, 72.49it/s, updates=999, loss=0.199]INFO:root:\n",
      "Checkpoint at 3100; train_loss=0.3896 valid_loss=0.4252\n",
      " 32%|███▏      | 3194/10000 [01:28<01:40, 67.47it/s, updates=999, loss=0.57] INFO:root:\n",
      "Checkpoint at 3200; train_loss=0.4159 valid_loss=0.4019\n",
      " 33%|███▎      | 3297/10000 [01:31<01:28, 75.71it/s, updates=999, loss=0.57] INFO:root:\n",
      "Checkpoint at 3300; train_loss=0.3904 valid_loss=0.3991\n",
      " 34%|███▍      | 3391/10000 [01:34<01:31, 71.88it/s, updates=999, loss=0.89] INFO:root:\n",
      "Checkpoint at 3400; train_loss=0.4171 valid_loss=0.4083\n",
      " 35%|███▍      | 3497/10000 [01:37<01:25, 75.84it/s, updates=999, loss=0.307]INFO:root:\n",
      "Checkpoint at 3500; train_loss=0.4440 valid_loss=0.3773\n",
      " 36%|███▌      | 3596/10000 [01:39<01:23, 76.86it/s, updates=999, loss=0.572]INFO:root:\n",
      "Checkpoint at 3600; train_loss=0.4215 valid_loss=0.3817\n",
      " 37%|███▋      | 3691/10000 [01:42<01:25, 73.65it/s, updates=999, loss=0.36] INFO:root:\n",
      "Checkpoint at 3700; train_loss=0.3795 valid_loss=0.3762\n",
      " 38%|███▊      | 3796/10000 [01:45<01:22, 75.33it/s, updates=999, loss=0.396]INFO:root:\n",
      "Checkpoint at 3800; train_loss=0.4175 valid_loss=0.3808\n",
      " 39%|███▉      | 3892/10000 [01:47<01:23, 73.07it/s, updates=999, loss=0.323]INFO:root:\n",
      "Checkpoint at 3900; train_loss=0.4045 valid_loss=0.3880\n",
      " 40%|███▉      | 3995/10000 [01:50<01:18, 76.98it/s, updates=999, loss=0.56] INFO:root:\n",
      "Checkpoint at 4000; train_loss=0.3918 valid_loss=0.3821\n",
      " 41%|████      | 4093/10000 [01:53<01:20, 73.19it/s, updates=999, loss=0.316]INFO:root:\n",
      "Checkpoint at 4100; train_loss=0.3925 valid_loss=0.3962\n",
      " 42%|████▏     | 4191/10000 [01:56<01:27, 66.17it/s, updates=999, loss=0.288]INFO:root:\n",
      "Checkpoint at 4200; train_loss=0.3828 valid_loss=0.3773\n",
      " 43%|████▎     | 4296/10000 [01:59<01:14, 76.50it/s, updates=999, loss=0.373]INFO:root:\n",
      "Checkpoint at 4300; train_loss=0.3933 valid_loss=0.4122\n",
      " 44%|████▍     | 4394/10000 [02:01<01:14, 75.72it/s, updates=999, loss=0.508]INFO:root:\n",
      "Checkpoint at 4400; train_loss=0.4072 valid_loss=0.3600\n",
      " 45%|████▍     | 4493/10000 [02:04<01:11, 76.56it/s, updates=999, loss=0.177]INFO:root:\n",
      "Checkpoint at 4500; train_loss=0.3877 valid_loss=0.3918\n",
      " 46%|████▌     | 4598/10000 [02:07<01:11, 75.76it/s, updates=999, loss=0.689]INFO:root:\n",
      "Checkpoint at 4600; train_loss=0.3885 valid_loss=0.3585\n",
      " 47%|████▋     | 4698/10000 [02:10<01:18, 67.48it/s, updates=999, loss=0.48] INFO:root:\n",
      "Checkpoint at 4700; train_loss=0.3723 valid_loss=0.3594\n",
      " 48%|████▊     | 4796/10000 [02:12<01:08, 75.44it/s, updates=999, loss=0.527]INFO:root:\n",
      "Checkpoint at 4800; train_loss=0.3757 valid_loss=0.3652\n",
      " 49%|████▉     | 4894/10000 [02:15<01:09, 73.66it/s, updates=999, loss=0.259]INFO:root:\n",
      "Checkpoint at 4900; train_loss=0.3667 valid_loss=0.3756\n",
      " 50%|████▉     | 4998/10000 [02:18<01:06, 74.74it/s, updates=999, loss=0.739] INFO:root:\n",
      "Checkpoint at 5000; train_loss=0.3570 valid_loss=0.3700\n",
      " 51%|█████     | 5095/10000 [02:20<01:07, 72.43it/s, updates=999, loss=0.241]INFO:root:\n",
      "Checkpoint at 5100; train_loss=0.3721 valid_loss=0.3521\n",
      " 52%|█████▏    | 5192/10000 [02:23<01:05, 73.52it/s, updates=999, loss=0.168]INFO:root:\n",
      "Checkpoint at 5200; train_loss=0.3626 valid_loss=0.3425\n",
      " 53%|█████▎    | 5290/10000 [02:26<01:04, 73.20it/s, updates=999, loss=0.381]INFO:root:\n",
      "Checkpoint at 5300; train_loss=0.3782 valid_loss=0.3907\n",
      " 54%|█████▍    | 5395/10000 [02:29<01:00, 75.75it/s, updates=999, loss=0.302] INFO:root:\n",
      "Checkpoint at 5400; train_loss=0.3968 valid_loss=0.3485\n",
      " 55%|█████▍    | 5492/10000 [02:31<01:01, 73.53it/s, updates=999, loss=0.108]INFO:root:\n",
      "Checkpoint at 5500; train_loss=0.3557 valid_loss=0.3534\n",
      " 56%|█████▌    | 5593/10000 [02:34<00:59, 73.95it/s, updates=999, loss=0.643]INFO:root:\n",
      "Checkpoint at 5600; train_loss=0.3745 valid_loss=0.3537\n",
      " 57%|█████▋    | 5690/10000 [02:37<00:58, 73.64it/s, updates=999, loss=0.268]INFO:root:\n",
      "Checkpoint at 5700; train_loss=0.3415 valid_loss=0.3762\n",
      " 58%|█████▊    | 5792/10000 [02:40<01:03, 66.57it/s, updates=999, loss=0.248] INFO:root:\n",
      "Checkpoint at 5800; train_loss=0.3179 valid_loss=0.3995\n",
      " 59%|█████▉    | 5896/10000 [02:43<00:55, 73.75it/s, updates=999, loss=0.375]INFO:root:\n",
      "Checkpoint at 5900; train_loss=0.3686 valid_loss=0.3497\n",
      " 60%|█████▉    | 5991/10000 [02:45<00:55, 72.67it/s, updates=999, loss=0.29] INFO:root:\n",
      "Checkpoint at 6000; train_loss=0.3572 valid_loss=0.3987\n",
      " 60%|██████    | 6000/10000 [02:47<04:07, 16.14it/s, updates=999, loss=0.384]INFO:root:End of epoch 0\n",
      " 61%|██████    | 6097/10000 [02:48<00:49, 78.23it/s, updates=999, loss=0.22] INFO:root:\n",
      "Checkpoint at 6100; train_loss=0.3463 valid_loss=0.3495\n",
      " 62%|██████▏   | 6192/10000 [02:51<00:54, 70.40it/s, updates=999, loss=0.189]INFO:root:\n",
      "Checkpoint at 6200; train_loss=0.3348 valid_loss=0.3479\n",
      " 63%|██████▎   | 6296/10000 [02:53<00:51, 71.48it/s, updates=999, loss=0.304]INFO:root:\n",
      "Checkpoint at 6300; train_loss=0.3056 valid_loss=0.3421\n",
      " 64%|██████▍   | 6395/10000 [02:57<00:54, 66.47it/s, updates=999, loss=0.147]INFO:root:\n",
      "Checkpoint at 6400; train_loss=0.3368 valid_loss=0.3629\n",
      " 65%|██████▍   | 6491/10000 [02:59<00:48, 71.84it/s, updates=999, loss=0.371] INFO:root:\n",
      "Checkpoint at 6500; train_loss=0.3147 valid_loss=0.3426\n",
      " 66%|██████▌   | 6596/10000 [03:02<00:45, 74.56it/s, updates=999, loss=0.209] INFO:root:\n",
      "Checkpoint at 6600; train_loss=0.3040 valid_loss=0.3519\n",
      " 67%|██████▋   | 6690/10000 [03:05<00:45, 73.43it/s, updates=999, loss=0.342]INFO:root:\n",
      "Checkpoint at 6700; train_loss=0.3377 valid_loss=0.3414\n",
      " 68%|██████▊   | 6794/10000 [03:08<00:42, 74.68it/s, updates=999, loss=0.191]INFO:root:\n",
      "Checkpoint at 6800; train_loss=0.3598 valid_loss=0.3447\n",
      " 69%|██████▉   | 6893/10000 [03:10<00:41, 74.81it/s, updates=999, loss=0.531] INFO:root:\n",
      "Checkpoint at 6900; train_loss=0.2955 valid_loss=0.3342\n",
      " 70%|██████▉   | 6994/10000 [03:13<00:38, 78.82it/s, updates=999, loss=0.445]INFO:root:\n",
      "Checkpoint at 7000; train_loss=0.3593 valid_loss=0.3208\n",
      " 71%|███████   | 7097/10000 [03:16<00:44, 65.50it/s, updates=999, loss=0.337]INFO:root:\n",
      "Checkpoint at 7100; train_loss=0.3235 valid_loss=0.3530\n",
      " 72%|███████▏  | 7193/10000 [03:19<00:39, 70.73it/s, updates=999, loss=0.189]INFO:root:\n",
      "Checkpoint at 7200; train_loss=0.3217 valid_loss=0.3559\n",
      " 73%|███████▎  | 7291/10000 [03:21<00:35, 77.23it/s, updates=999, loss=0.322] INFO:root:\n",
      "Checkpoint at 7300; train_loss=0.3302 valid_loss=0.3265\n",
      " 74%|███████▍  | 7398/10000 [03:24<00:32, 81.11it/s, updates=999, loss=0.508]INFO:root:\n",
      "Checkpoint at 7400; train_loss=0.3157 valid_loss=0.3370\n",
      " 75%|███████▍  | 7497/10000 [03:27<00:32, 77.49it/s, updates=999, loss=0.593]INFO:root:\n",
      "Checkpoint at 7500; train_loss=0.3026 valid_loss=0.3422\n",
      " 76%|███████▌  | 7596/10000 [03:30<00:31, 77.14it/s, updates=999, loss=0.417]INFO:root:\n",
      "Checkpoint at 7600; train_loss=0.3094 valid_loss=0.3215\n",
      " 77%|███████▋  | 7695/10000 [03:32<00:29, 77.33it/s, updates=999, loss=0.396]INFO:root:\n",
      "Checkpoint at 7700; train_loss=0.2870 valid_loss=0.3356\n",
      " 78%|███████▊  | 7792/10000 [03:35<00:30, 72.62it/s, updates=999, loss=0.478] INFO:root:\n",
      "Checkpoint at 7800; train_loss=0.2956 valid_loss=0.3569\n",
      " 79%|███████▉  | 7897/10000 [03:38<00:28, 75.00it/s, updates=999, loss=0.49]  INFO:root:\n",
      "Checkpoint at 7900; train_loss=0.2904 valid_loss=0.3330\n",
      " 80%|███████▉  | 7993/10000 [03:40<00:27, 73.90it/s, updates=999, loss=0.185] INFO:root:\n",
      "Checkpoint at 8000; train_loss=0.3102 valid_loss=0.3400\n",
      " 81%|████████  | 8098/10000 [03:44<00:52, 36.14it/s, updates=999, loss=0.391]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-50eb0f1d748f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-366ec0c0ee61>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, valid_loader, steps, checkpoint)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                         \u001b[0;31m#👇\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-366ec0c0ee61>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, valid_loader)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torchtext/experimental/datasets/text_classification.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torchtext/experimental/functional.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(txt_input)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtxt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtxt_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torchtext/experimental/functional.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(tok_iter)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvocab_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtok_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/libs/miniconda3/envs/torch-3.7/lib/python3.7/site-packages/torchtext/experimental/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvocab_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtok_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=Batch.collate_fn)\n",
    "valid_loader = DataLoader(valid, batch_size=BATCH_SIZE, shuffle=False, collate_fn=Batch.collate_fn)\n",
    "\n",
    "trainer = Trainer(model=model)\n",
    "trainer.train(train_loader=train_loader, valid_loader=valid_loader, steps=10000, checkpoint=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
